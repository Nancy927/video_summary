import os
import numpy as np
from PIL import Image
import torch
from torchvision import transforms, models
import onnxruntime as ort
from utils.video_summarizer import get_summary_frame_indices

# âœ… å›¾åƒé¢„å¤„ç†
preprocess = transforms.Compose([
    transforms.Resize((224, 224)),
    transforms.ToTensor(),
    transforms.Normalize(mean=[0.485, 0.456, 0.406],
                         std=[0.229, 0.224, 0.225]),
])

# âœ… åŠ è½½ ONNX æ¨¡å‹
def load_onnx_model(onnx_path):
    return ort.InferenceSession(onnx_path)

# âœ… è¯»å–å¸§å›¾åƒè·¯å¾„ï¼ˆæŒ‰é¡ºåºï¼‰
def load_frame_paths(frame_dir):
    frame_files = sorted([f for f in os.listdir(frame_dir) if f.endswith('.jpg')])
    return [os.path.join(frame_dir, f) for f in frame_files]

# âœ… æå– clip ç‰¹å¾åºåˆ—ï¼ˆæ¯ä¸ªclipå« T å¸§ï¼‰
def extract_clip_batches(frame_paths, clip_len=5, stride=1):
    clips = []
    indices = []
    for i in range(0, len(frame_paths) - clip_len + 1, stride):
        clip = frame_paths[i:i+clip_len]
        clips.append(clip)
        indices.append(i + clip_len // 2)  # å–ä¸­é—´å¸§ç´¢å¼•ä½œä¸ºä»£è¡¨å¸§
    return clips, indices

# âœ… å¯¹å•ä¸ª clip æå–é¢„å¤„ç†åçš„ Tensor
def process_clip(clip_paths):
    clip_images = [preprocess(Image.open(p).convert('RGB')) for p in clip_paths]
    return torch.stack(clip_images).unsqueeze(0).numpy()  # (1, T, 3, 224, 224)

# âœ… ä¸»å‡½æ•°ï¼šè¾“å…¥å¸§ç›®å½•ï¼Œè¾“å‡ºå…³é”®å¸§å¾—åˆ†åˆ—è¡¨
def predict_keyframes_with_onnx(frame_dir, model_weights_path, audio_path=None, text_score_map=None, device='cuda' if torch.cuda.is_available() else 'cpu'):
    """
    æ›¿ä»£ ONNX æ¨ç†ï¼šè°ƒç”¨ get_summary_frame_indices æ‰§è¡Œå¤šæ¨¡æ€èåˆï¼Œè¿”å›å…³é”®å¸§ç´¢å¼•ä¸è™šæ‹Ÿå¾—åˆ†ï¼ˆscore=1.0ï¼‰ã€‚
    è¿”å›å€¼æ ¼å¼ä¿æŒä¸æ—§ç‰ˆç›¸åŒï¼š[(index1, score1), (index2, score2), ...]
    """
    import cv2

    print(f"[INFO] ğŸš€ ä½¿ç”¨æœ¬åœ°æ¨¡å‹æ¨ç†æ›¿ä»£ ONNX: {model_weights_path}")
    # è·å–å¸§ç¼–å· â†’ è§†é¢‘è·¯å¾„
    frame_paths = load_frame_paths(frame_dir)
    if not frame_paths:
        raise ValueError("âŒ å¸§ç›®å½•ä¸ºç©º")
    
    # æ ¹æ®å¸§è·¯å¾„åˆæˆ video_path
    frame_dir_path = os.path.abspath(frame_dir)
    video_path = os.path.join(os.path.dirname(frame_dir_path), "temp_input.mp4")

    # ä½¿ç”¨ OpenCV åˆæˆè§†é¢‘æ–‡ä»¶ï¼Œä¾¿äºè°ƒç”¨ video_summarizer æ¥å£
    fps = 30
    frame = cv2.imread(frame_paths[0])
    h, w, _ = frame.shape
    fourcc = cv2.VideoWriter_fourcc(*'mp4v')
    out = cv2.VideoWriter(video_path, fourcc, fps, (w, h))

    for path in frame_paths:
        img = cv2.imread(path)
        out.write(img)
    out.release()
    print(f"[INFO] âœ… åˆæˆä¸´æ—¶è§†é¢‘å®Œæˆ: {video_path}")

    # æ‰§è¡Œæ›¿ä»£æ¨ç†
    key_indices = get_summary_frame_indices(
        video_path=video_path,
        audio_path=audio_path,
        text_score_map=text_score_map or {},
        model_weights_path=model_weights_path,
        device=device
    )

    print(f"[INFO] âœ… å¾—åˆ°å…³é”®å¸§ç´¢å¼•æ•°é‡: {len(key_indices)}")

    return [(idx, 1.0) for idx in key_indices]  # ä¿æŒæ¥å£ä¸€è‡´