import os
import numpy as np
import cv2
import torch
from moviepy.editor import VideoFileClip, concatenate_videoclips
from utils.extract_audio import extract_audio_from_video
from utils.module5_multimodal_fusion import (
    extract_frames_per_second,
    extract_visual_features,
    extract_audio_feature,
    compute_final_scores_cnn_lstm_attention,
    remove_similar_frames,
    dynamic_topk_selection,
    select_topkeyframes_by_score,
    CNNLSTMAttention
)

def generate_summary_from_indices(video_path, frame_indices, output_path, clip_margin=2.0, min_gap=2.0):
    """
    æ ¹æ®å…³é”®å¸§ç¼–å·ç”Ÿæˆæ‘˜è¦è§†é¢‘ï¼ˆåŸºäºå¸§å·ç›´æ¥æ¢ç®—æ—¶é—´æˆ³ï¼Œè‡ªåŠ¨æå– fpsï¼‰ã€‚
    """
    if not frame_indices:
        raise ValueError("å…³é”®å¸§ç´¢å¼•åˆ—è¡¨ä¸ºç©º")

    frame_indices = sorted(set(frame_indices))

    # è‡ªåŠ¨è·å–è§†é¢‘å¸§ç‡
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    cap.release()

    if fps <= 0:
        raise ValueError("æ— æ³•ä»è§†é¢‘ä¸­æå–æœ‰æ•ˆçš„ fps")

    # è®¡ç®—å…³é”®å¸§å¯¹åº”çš„æ—¶é—´æˆ³
    timestamps = [idx / fps for idx in frame_indices]

    def merge_times_to_segments(times, clip_margin, duration):
        if not times:
            return []
        segments = []
        times = sorted(times)
        start = max(0, times[0] - clip_margin)
        end = min(duration, times[0] + clip_margin)
        for t in times[1:]:
            new_start = max(0, t - clip_margin)
            new_end = min(duration, t + clip_margin)
            if new_start <= end:
                end = max(end, new_end)
            else:
                segments.append((start, end))
                start, end = new_start, new_end
        segments.append((start, end))
        return segments

    try:
        video = VideoFileClip(video_path)
        duration = video.duration
        segments = [video.subclip(start, end) for start, end in merge_times_to_segments(timestamps, clip_margin, duration)]

        if not segments:
            raise ValueError("æœªç”Ÿæˆä»»ä½•æœ‰æ•ˆæ‘˜è¦ç‰‡æ®µ")

        final_clip = concatenate_videoclips(segments)
        os.makedirs(os.path.dirname(output_path), exist_ok=True)

        # ğŸ”§ ä¼˜åŒ–å‚æ•°é¿å…å†…å­˜å´©æºƒ
        final_clip.write_videofile(
            output_path,
            codec="libx264",
            audio_codec="aac",
            fps=15,
            bitrate="800k",
            threads=2,
            temp_audiofile='temp-audio.m4a',
            remove_temp=True
        )

        final_clip.close()
        video.close()

    except Exception as e:
        print(f"[ERROR] âŒ æ‘˜è¦è§†é¢‘åˆæˆå¤±è´¥: {e}")
        raise RuntimeError("è§†é¢‘åˆæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯ï¼Œè¯·æ£€æŸ¥èµ„æºå ç”¨æˆ–è§†é¢‘å†…å®¹ã€‚")

    # æ¸…ç†å¸§å›¾åƒæ–‡ä»¶å¤¹
    try:
        video_id = os.path.splitext(os.path.basename(video_path))[0]
        frame_dir = os.path.join('./outputs/frames', video_id)
        if os.path.exists(frame_dir):
            for f in os.listdir(frame_dir):
                file_path = os.path.join(frame_dir, f)
                if os.path.isfile(file_path):
                    os.remove(file_path)
            os.rmdir(frame_dir)
    except Exception as e:
        print(f"[WARN] âŒ æ¸…ç†å¸§å›¾åƒç›®å½•å¤±è´¥: {e}")

    return output_path


def extract_frame_timestamps(video_path, keyframe_paths, resize=(224, 224)):
    """
    åœ¨åŸè§†é¢‘ä¸­å®šä½å…³é”®å¸§å¯¹åº”æ—¶é—´æˆ³ï¼ˆä½¿ç”¨å›¾åƒåŒ¹é…ï¼‰ã€‚
    è¿”å›æ¯ä¸ªå…³é”®å¸§çš„æ—¶é—´ï¼ˆç§’ï¼‰ã€‚
    """
    cap = cv2.VideoCapture(video_path)
    fps = cap.get(cv2.CAP_PROP_FPS)
    frame_count = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))

    keyframes = [cv2.resize(cv2.imread(p), resize) for p in keyframe_paths if cv2.imread(p) is not None]
    key_timestamps = []

    for idx in range(frame_count):
        success, frame = cap.read()
        if not success:
            break
        resized = cv2.resize(frame, resize)

        for kf in keyframes:
            diff = np.mean(np.abs(resized.astype(np.float32) - kf.astype(np.float32)))
            if diff < 10:  # åŒ¹é…é˜ˆå€¼å¯è°ƒ
                t = idx / fps
                key_timestamps.append(t)
                keyframes.remove(kf)  # åŒ¹é…æˆåŠŸå°±ç§»é™¤
                break

        if not keyframes:
            break

    cap.release()
    return sorted(key_timestamps)

def get_summary_frame_indices(video_path, audio_path, text_score_map,
                              model_weights_path, device="cuda" if torch.cuda.is_available() else "cpu"):
    """
    æ•´åˆè§†è§‰ã€éŸ³é¢‘ã€æ–‡æœ¬å¾—åˆ†ï¼Œè¿”å›å…³é”®å¸§çš„ç´¢å¼•ç¼–å·åˆ—è¡¨ã€‚
    å‚æ•°:
        video_path: è§†é¢‘æ–‡ä»¶è·¯å¾„
        audio_path: éŸ³é¢‘æ–‡ä»¶è·¯å¾„ï¼ˆ.wavï¼‰
        text_score_map: dict[str, float]ï¼Œè§†é¢‘ ID -> æ–‡æœ¬å¾—åˆ†ï¼ˆä¾‹å¦‚ TextRank è¾“å‡ºï¼‰
        model_weights_path: æ¨¡å‹ .pth æ–‡ä»¶è·¯å¾„
        device: é»˜è®¤è‡ªåŠ¨ä½¿ç”¨ GPU/CPU
    è¿”å›:
        List[int]ï¼Œå…³é”®å¸§ç´¢å¼•
    """
    print("[DEBUG] âœ… æ­£åœ¨ä½¿ç”¨ get_summary_frame_indices æ¨¡å‹æ¨ç†...")
    # åŠ è½½æ³¨æ„åŠ›æ¨¡å‹
    model = CNNLSTMAttention().to(device)
    model.load_state_dict(torch.load(model_weights_path, map_location=device))
    model.eval()

    # æ¯ç§’æå–ä¸€å¸§å›¾åƒ
    raw_images, frame_indices = extract_frames_per_second(video_path)  # æ–°æ–¹æ³•

    # æå–è§†è§‰ç‰¹å¾
    visual_feats = extract_visual_features(raw_images)  # shape: (1, N, 576)

    # æå–éŸ³é¢‘ç‰¹å¾
    audio_feat = extract_audio_feature(audio_path)  # shape: (64,)

    # æ–‡æœ¬å¾—åˆ†ï¼ˆä» map ä¸­è·å– video_idï¼‰
    video_id = os.path.splitext(os.path.basename(video_path))[0]
    text_score_scalar = text_score_map.get(video_id, 0.5)

    # èåˆæ‰“åˆ†ï¼ˆè§†è§‰æ³¨æ„åŠ›è¾“å‡º + éŸ³é¢‘ç›¸ä¼¼åº¦ + æ–‡æœ¬å¾—åˆ†ï¼‰
    fused_scores = compute_final_scores_cnn_lstm_attention(
        visual_seq=visual_feats,
        audio_feat=audio_feat,
        text_score_scalar=text_score_scalar
    )

    # ç›¸ä¼¼å¸§å»é‡ï¼ˆè¿”å›å›¾åƒã€ç´¢å¼•ã€ç‰¹å¾ï¼‰
    filtered_images, keep_indices, filtered_feats = remove_similar_frames(
        raw_images, visual_feats, threshold=0.92
    )

    # æ ¹æ®èåˆå¾—åˆ†åŠ¨æ€é€‰æ‹© K
    filtered_scores = fused_scores[keep_indices]
    topk = dynamic_topk_selection(filtered_scores, min_frames=10, max_frames=30)
    selected_indices = select_topkeyframes_by_score(filtered_scores, topk=topk)

    # æ˜ å°„å›åŸå§‹å¸§ç´¢å¼•ï¼ˆä»¥ä¾¿åç»­æ¨ç®—æ—¶é—´æˆ³ï¼‰
    final_indices = [frame_indices[keep_indices[i]] for i in selected_indices]  # å…³é”®ä¿®æ”¹ç‚¹

    return final_indices
